{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to experiment with the actor-critic algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    "### 1. __TensorFlow:__\n",
    "https://www.tensorflow.org/get_started/os_setup. GPU is not needed.\n",
    "### 2. __OpenAI Gym:__  \n",
    "OpenAI Gym - https://gym.openai.com/docs. Please, use version 0.10.5.\n",
    "### 3. __TensorFlow-probabilities:__<br>\n",
    "https://www.tensorflow.org/probability/install. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import everything we need and write auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import gym\n",
    "print(gym.__version__)\n",
    "\n",
    "import logz\n",
    "import os\n",
    "import time\n",
    "import inspect\n",
    "from multiprocessing import Process\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path[\"reward\"])\n",
    "\n",
    "def setup_logger(logdir, locals_):\n",
    "    # Configure output directory for logging\n",
    "    logz.configure_output_dir(logdir)\n",
    "    # Log experimental parameters\n",
    "    args = inspect.getargspec(train_PG)[0]\n",
    "    params = {k: locals_[k] if k in locals_ else None for k in args}\n",
    "    logz.save_params(params)\n",
    "    \n",
    "x=[]\n",
    "y=[]\n",
    "fig=None\n",
    "def plot(t, mean_r):\n",
    "    global x\n",
    "    global y \n",
    "    if t==0:\n",
    "        x = []\n",
    "        y = []\n",
    "        fig=plt.figure()\n",
    "    x.append(t)\n",
    "    y.append(mean_r)\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(x,y,label='mean_reward')\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Recall the necessary formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the objective function is calcualted to determine the gradient of the strategy using the following formula:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta)\\approx\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{T} \\nabla_\\theta\\log\\pi_\\theta(a_{it},s_{it}) A^\\pi(s_t,a_t).$$\n",
    "\n",
    "Here we evaluate the utility function of the action through the advantage function $ A $, which, to reduce the variance of the solution, we present in the form of the difference between the critic's estimates and the base function $B(s)=V_v^\\pi(s)$:\n",
    "\n",
    "$$A^\\pi(s_t,a_t)\\approx r(a_t,s_t)+\\gamma V_v^\\pi(s_{t+1})-V_v^\\pi(s_{it})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critic who evaluates reward function minimizes standard error for TD metric $y_t=r(a_t,s_t)+\\gamma V^\\pi(s_{t+1})$:\n",
    "\n",
    "$$\\min_v \\sum_{i,t}(V_v^\\pi(s_{it})-y_{it})^2.$$\n",
    "\n",
    "In order to speed up the work, we repeat the following two steps:\n",
    "\n",
    "1. Update the metric with the old value of the utility function.\n",
    "2. We take several gradient steps to update the utility function.\n",
    "\n",
    "Those. The criticâ€™s training is iterative: we update the criterion and then update the function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an approximator.\n",
    "\n",
    "We need to create a simple approximator in the form of several layers (__n_layers__) of fully connected neural networks (__tf.layers.dense__). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_placeholder, output_size, scope, n_layers, size, activation=tf.tanh, output_activation=None):\n",
    "    \"\"\"\n",
    "        Builds a feedforward neural network\n",
    "        \n",
    "        arguments:\n",
    "            input_placeholder: placeholder variable for the state (batch_size, input_size)\n",
    "            output_size: size of the output layer\n",
    "            scope: variable scope of the network\n",
    "            n_layers: number of hidden layers\n",
    "            size: dimension of the hidden layer\n",
    "            activation: activation of the hidden layers\n",
    "            output_activation: activation of the ouput layers\n",
    "        returns:\n",
    "            output placeholder of the network (the result of a forward pass) \n",
    "        Hint: use tf.layers.dense    \n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope + \"1\"):\n",
    "        output_placeholder = input_placeholder\n",
    "        for _ in range(n_layers):\n",
    "            output_placeholder = tf.layers.dense(output_placeholder, size, activation=activation)\n",
    "        output_placeholder = tf.layers.dense(output_placeholder, output_size, activation=output_activation)\n",
    "    return output_placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the core Agent class. First we define its constructor, the parameters of which are taken from the dictionary __computation_graph_args__. Parameters __computation_graph_args__ are auxiliary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, computation_graph_args, sample_trajectory_args, estimate_advantage_args, scope):\n",
    "        super(Agent, self).__init__()\n",
    "        self.ob_dim = computation_graph_args['ob_dim']\n",
    "        self.ac_dim = computation_graph_args['ac_dim']\n",
    "        self.discrete = computation_graph_args['discrete']\n",
    "        self.size = computation_graph_args['size']\n",
    "        self.n_layers = computation_graph_args['n_layers']\n",
    "        self.learning_rate = computation_graph_args['learning_rate']\n",
    "        self.num_target_updates = computation_graph_args['num_target_updates']\n",
    "        self.num_grad_steps_per_target_update = computation_graph_args['num_grad_steps_per_target_update']\n",
    "\n",
    "        self.animate = sample_trajectory_args['animate']\n",
    "        self.max_path_length = sample_trajectory_args['max_path_length']\n",
    "        self.min_timesteps_per_batch = sample_trajectory_args['min_timesteps_per_batch']\n",
    "\n",
    "        self.gamma = estimate_advantage_args['gamma']\n",
    "        self.normalize_advantages = estimate_advantage_args['normalize_advantages']\n",
    "        self.scope = scope\n",
    "        \n",
    "    def init_tf_sess(self):\n",
    "        tf_config = tf.ConfigProto(inter_op_parallelism_threads=1, intra_op_parallelism_threads=1)\n",
    "        tf_config.gpu_options.allow_growth = True # may need if using GPU\n",
    "        self.sess = tf.Session(config=tf_config)\n",
    "        self.sess.__enter__() # equivalent to `with self.sess:`\n",
    "        tf.global_variables_initializer().run() #pylint: disable=E1101\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        \"\"\"\n",
    "            Placeholders for batch batch observations / actions / advantages in actor critic\n",
    "            loss function.\n",
    "            See Agent.build_computation_graph for notation\n",
    "\n",
    "            returns:\n",
    "                sy_ob_no: placeholder for observations\n",
    "                sy_ac_na: placeholder for actions\n",
    "                sy_adv_n: placeholder for advantages\n",
    "        \"\"\"\n",
    "        sy_ob_no = tf.placeholder(shape=[None, self.ob_dim], name=\"ob\", dtype=tf.float32)\n",
    "        if self.discrete:\n",
    "            sy_ac_na = tf.placeholder(shape=[None], name=\"ac\", dtype=tf.int32) \n",
    "        else:\n",
    "            sy_ac_na = tf.placeholder(shape=[None, self.ac_dim], name=\"ac\", dtype=tf.float32) \n",
    "        \n",
    "        sy_adv_n = tf.placeholder(shape=[None],name=\"advantage\",dtype=tf.float32)\n",
    "        return sy_ob_no, sy_ac_na, sy_adv_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Implementation of the algorithm of the actor.\n",
    "\n",
    "First, we need to add a code to determine the action for the current observation (__sy_ob_no__). This operation varies for continuous and discrete environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_forward_pass(self, sy_ob_no):\n",
    "    \"\"\" Constructs the symbolic operation for the policy network outputs,\n",
    "        which are the parameters of the policy distribution p(a|s)\n",
    "        arguments:\n",
    "            sy_ob_no: (batch_size, self.ob_dim)\n",
    "        returns:\n",
    "            the parameters of the policy.\n",
    "            if discrete, the parameters are the logits of a categorical distribution\n",
    "                over the actions\n",
    "                sy_logits_na: (batch_size, self.ac_dim)\n",
    "            if continuous, the parameters are a tuple (mean, log_std) of a Gaussian\n",
    "                distribution over actions. log_std should just be a trainable\n",
    "                variable, not a network output.\n",
    "                sy_mean: (batch_size, self.ac_dim)\n",
    "                sy_logstd: (self.ac_dim,)\n",
    "        Hint: use the 'build_mlp' function to output the logits (in the discrete case)\n",
    "            and the mean (in the continuous case).\n",
    "            Pass in self.n_layers for the 'n_layers' argument, and\n",
    "            pass in self.size for the 'size' argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    if self.discrete:\n",
    "        sy_logits_na = build_mlp(sy_ob_no,self.ac_dim,self.scope+\"0\",self.n_layers,self.size, activation=tf.nn.relu)\n",
    "        return sy_logits_na\n",
    "    else:\n",
    "        sy_mean = build_mlp(sy_ob_no,self.ac_dim,self.scope+\"0\",self.n_layers,self.size,activation=tf.nn.relu)\n",
    "        sy_logstd = tf.Variable(np.zeros(self.ac_dim),name=\"sy_logstd\")\n",
    "        return (sy_mean, sy_logstd)\n",
    "    \n",
    "Agent.policy_forward_pass = policy_forward_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select the needed action from the distribution, which is given to us by the strategy. For a discrete and continuous case, we use different methods (__tf.squeeze__ and __tf.random_normal__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action(self, policy_parameters):\n",
    "    \"\"\" Constructs a symbolic operation for stochastically sampling from the policy\n",
    "        distribution\n",
    "        arguments:\n",
    "            policy_parameters\n",
    "                if discrete: logits of a categorical distribution over actions \n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "                if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "        returns:\n",
    "            sy_sampled_ac: \n",
    "                if discrete: (batch_size,)\n",
    "                if continuous: (batch_size, self.ac_dim)\n",
    "        Hint: for the continuous case, use the reparameterization trick:\n",
    "             The output from a Gaussian distribution with mean 'mu' and std 'sigma' is\n",
    "\n",
    "                  mu + sigma * z,         z ~ N(0, I)\n",
    "\n",
    "             This reduces the problem to just sampling z. (Hint: use tf.random_normal!)\n",
    "    \"\"\"\n",
    "    if self.discrete:\n",
    "        sy_logits_na = policy_parameters\n",
    "        # use tf.squeeze (Removes dimensions of size 1 from the shape of a tensor.)\n",
    "        sy_sampled_ac = tf.squeeze(tf.multinomial(sy_logits_na, 1), axis=1)\n",
    "    else:\n",
    "        sy_mean, sy_logstd = policy_parameters\n",
    "        exp=tf.cast(tf.exp(sy_logstd), dtype=tf.float32)\n",
    "        sy_sampled_ac = sy_mean + exp * tf.random_normal(shape=tf.shape(sy_mean))\n",
    "    return sy_sampled_ac\n",
    "\n",
    "Agent.sample_action = sample_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the logarithm of the probability of the action for use in the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_prob(self, policy_parameters, sy_ac_na):\n",
    "    \"\"\" Constructs a symbolic operation for computing the log probability of a set of actions\n",
    "        that were actually taken according to the policy\n",
    "        arguments:\n",
    "            policy_parameters\n",
    "                if discrete: logits of a categorical distribution over actions \n",
    "                    sy_logits_na: (batch_size, self.ac_dim)\n",
    "                if continuous: (mean, log_std) of a Gaussian distribution over actions\n",
    "                    sy_mean: (batch_size, self.ac_dim)\n",
    "                    sy_logstd: (self.ac_dim,)\n",
    "            sy_ac_na: \n",
    "                if discrete: (batch_size,)\n",
    "                if continuous: (batch_size, self.ac_dim)\n",
    "        returns:\n",
    "            sy_logprob_n: (batch_size)\n",
    "        Hint:\n",
    "            For the discrete case, use the log probability under a categorical distribution.\n",
    "            For the continuous case, use the log probability under a multivariate gaussian.\n",
    "    \"\"\"\n",
    "    if self.discrete:\n",
    "        sy_logits_na = policy_parameters\n",
    "        # use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "        sy_logprob_n = - tf.nn.sparse_softmax_cross_entropy_with_logits(labels=sy_ac_na,logits=sy_logits_na)\n",
    "    else:\n",
    "        sy_mean, sy_logstd = policy_parameters\n",
    "        # multivariate gaussian (tf.distributions.Normal)\n",
    "        exp=tf.cast(tf.exp(sy_logstd),dtype=tf.float32)\n",
    "        probabilities = tf.distributions.Normal(sy_mean, exp).prob(sy_ac_na)\n",
    "        sy_logprob_n = tf.log(tf.reduce_prod(probabilities, axis=1))\n",
    "    return sy_logprob_n\n",
    "\n",
    "Agent.get_log_prob = get_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the code for updating the parameters of the actor in the session. Here __actor_update_op__ is the optimizer, which we define later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor(self, ob_no, ac_na, adv_n):\n",
    "    \"\"\" \n",
    "        Update the parameters of the policy.\n",
    "\n",
    "        arguments:\n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            ac_na: shape: (sum_of_path_lengths).\n",
    "            adv_n: shape: (sum_of_path_lengths). A single vector for the estimated\n",
    "                advantages whose length is the sum of the lengths of the paths\n",
    "\n",
    "        returns:\n",
    "            nothing\n",
    "\n",
    "    \"\"\"\n",
    "    self.sess.run(self.actor_update_op,feed_dict={self.sy_ob_no: ob_no, self.sy_ac_na: ac_na, self.sy_adv_n: adv_n})\n",
    "    \n",
    "Agent.update_actor = update_actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Code for the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by evaluating the advantage function (see the formulas at the beginning of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_advantage(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "    \"\"\"\n",
    "        Estimates the advantage function value for each timestep.\n",
    "\n",
    "        let sum_of_path_lengths be the sum of the lengths of the paths sampled from \n",
    "            Agent.sample_trajectories\n",
    "\n",
    "        arguments:\n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n",
    "            re_n: length: sum_of_path_lengths. Each element in re_n is a scalar containing\n",
    "                the reward for each timestep\n",
    "            terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n",
    "                at that timestep of 0 if the episode did not end\n",
    "\n",
    "        returns:\n",
    "            adv_n: shape: (sum_of_path_lengths). A single vector for the estimated \n",
    "                advantages whose length is the sum of the lengths of the paths\n",
    "    \"\"\"\n",
    "    # First, estimate the Q value as Q(s, a) = r(s, a) + gamma*V(s')\n",
    "    # To get the advantage, subtract the V(s) to get A(s, a) = Q(s, a) - V(s)\n",
    "    # This requires calling the critic twice --- to obtain V(s') when calculating Q(s, a),\n",
    "    # and V(s) when subtracting the baseline\n",
    "    # Note: don't forget to use terminal_n to cut off the V(s') term when computing Q(s, a)\n",
    "    # otherwise the values will grow without bound.\n",
    "\n",
    "    adv_n = []\n",
    "    # calculate the value of the utility of the next state s' with the use of critic\n",
    "    v_s_tp1_n = self.sess.run(self.critic_prediction, feed_dict={self.sy_ob_no: next_ob_no})\n",
    "    # calculate the value of the utility of the current state s with the use of critic\n",
    "    v_s_t_n = self.sess.run(self.critic_prediction, feed_dict={self.sy_ob_no: ob_no})\n",
    "    # calculate the value Q (s, a) = r (s, a) + gamma * V (s')\n",
    "    q_n = re_n + self.gamma * v_s_tp1_n * (1 - terminal_n)\n",
    "    # A(s, a) = Q(s, a) - V(s)\n",
    "    adv_n = q_n - v_s_t_n\n",
    "\n",
    "    eps = 0.000000001 # not to devide by 0\n",
    "\n",
    "    if self.normalize_advantages:\n",
    "        # calculate the average and divide by the variance\n",
    "        adv_n = (adv_n - np.mean(adv_n)) / (np.std(adv_n) + eps)\n",
    "    return adv_n\n",
    "\n",
    "Agent.estimate_advantage = estimate_advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is necessary to write how we update the parameters of the critic in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(self, ob_no, next_ob_no, re_n, terminal_n):\n",
    "    \"\"\"\n",
    "        Update the parameters of the critic.\n",
    "\n",
    "        let sum_of_path_lengths be the sum of the lengths of the paths sampled from\n",
    "            Agent.sample_trajectories\n",
    "        let num_paths be the number of paths sampled from Agent.sample_trajectories\n",
    "\n",
    "        arguments:\n",
    "            ob_no: shape: (sum_of_path_lengths, ob_dim)\n",
    "            next_ob_no: shape: (sum_of_path_lengths, ob_dim). The observation after taking one step forward\n",
    "            re_n: length: sum_of_path_lengths. Each element in re_n is a scalar containing\n",
    "                the reward for each timestep\n",
    "            terminal_n: length: sum_of_path_lengths. Each element in terminal_n is either 1 if the episode ended\n",
    "                at that timestep of 0 if the episode did not end\n",
    "\n",
    "        returns:\n",
    "            nothing\n",
    "    \"\"\"\n",
    "    # Use a bootstrapped target values to update the critic\n",
    "    # Compute the target values r(s, a) + gamma*V(s') by calling the critic to compute V(s')\n",
    "    # In total, take n=self.num_grad_steps_per_target_update*self.num_target_updates gradient update steps\n",
    "    # Every self.num_grad_steps_per_target_update steps, recompute the target values\n",
    "    # by evaluating V(s') on the updated critic\n",
    "    # Note: don't forget to use terminal_n to cut off the V(s') term when computing the target\n",
    "    # otherwise the values will grow without bound.\n",
    "\n",
    "    for i in range(self.num_target_updates):\n",
    "        target_n = []\n",
    "        # calculate the value of the utility of the next state s' with the use of critic\n",
    "        v_s_tp1_n = self.sess.run(self.critic_prediction, feed_dict={self.sy_ob_no: next_ob_no})\n",
    "        # consider the value of the time difference index r (s, a) + gamma * V (s') if this is not a terminal state\n",
    "        target_n = re_n + self.gamma * v_s_tp1_n * (1 - terminal_n)\n",
    "\n",
    "        for j in range(self.num_grad_steps_per_target_update):\n",
    "            self.sess.run(self.critic_update_op, feed_dict={self.sy_target_n: target_n,self.sy_ob_no: ob_no})\n",
    "\n",
    "Agent.update_critic = update_critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Combination of methods\n",
    "\n",
    "Now let's combine all the previous methods into workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_computation_graph(self):\n",
    "    \"\"\"\n",
    "        Notes on notation:\n",
    "\n",
    "        Symbolic variables have the prefix sy_, to distinguish them from the numerical values\n",
    "        that are computed later in the function\n",
    "\n",
    "        Prefixes and suffixes:\n",
    "        ob - observation \n",
    "        ac - action\n",
    "        _no - this tensor should have shape (batch self.size /n/, observation dim)\n",
    "        _na - this tensor should have shape (batch self.size /n/, action dim)\n",
    "        _n  - this tensor should have shape (batch self.size /n/)\n",
    "\n",
    "        Note: batch self.size /n/ is defined at runtime, and until then, the shape for that axis\n",
    "        is None\n",
    "\n",
    "        ----------------------------------------------------------------------------------\n",
    "        loss: a function of self.sy_logprob_n and self.sy_adv_n that we will differentiate\n",
    "            to get the policy gradient.\n",
    "    \"\"\"\n",
    "    self.sy_ob_no, self.sy_ac_na, self.sy_adv_n = self.define_placeholders()\n",
    "\n",
    "    # The policy takes in an observation and produces a distribution over the action space\n",
    "    self.policy_parameters = self.policy_forward_pass(self.sy_ob_no)\n",
    "\n",
    "    # We can sample actions from this action distribution.\n",
    "    # This will be called in Agent.sample_trajectory() where we generate a rollout.\n",
    "    self.sy_sampled_ac = self.sample_action(self.policy_parameters)\n",
    "\n",
    "    # We can also compute the logprob of the actions that were actually taken by the policy\n",
    "    # This is used in the loss function.\n",
    "    self.sy_logprob_n = self.get_log_prob(self.policy_parameters, self.sy_ac_na)\n",
    "\n",
    "    actor_loss = tf.reduce_sum(-self.sy_logprob_n * self.sy_adv_n)\n",
    "    self.actor_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(actor_loss)\n",
    "\n",
    "    # define the critic\n",
    "    self.critic_prediction = tf.squeeze(build_mlp(\n",
    "                            self.sy_ob_no,\n",
    "                            1,\n",
    "                            self.scope + \"1\",\n",
    "                            n_layers=self.n_layers,\n",
    "                            size=self.size))\n",
    "    self.sy_target_n = tf.placeholder(shape=[None], name=\"critic_target\", dtype=tf.float32)\n",
    "    self.critic_loss = tf.losses.mean_squared_error(self.sy_target_n, self.critic_prediction)\n",
    "    self.critic_update_op = tf.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "\n",
    "Agent.build_computation_graph = build_computation_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add  a couple more methods  to the agent of working with statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(self, itr, env):\n",
    "    # Collect paths until we have enough timesteps\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    while True:\n",
    "        animate_this_episode=(len(paths)==0 and (itr % 10 == 0) and self.animate)\n",
    "        path = self.sample_trajectory(env, animate_this_episode)\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += pathlength(path)\n",
    "        if timesteps_this_batch > self.min_timesteps_per_batch:\n",
    "            break\n",
    "    return paths, timesteps_this_batch\n",
    "\n",
    "def sample_trajectory(self, env, animate_this_episode):\n",
    "    ob = env.reset()\n",
    "    obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if animate_this_episode:\n",
    "            env.render()\n",
    "            time.sleep(0.1)\n",
    "        obs.append(ob)\n",
    "        ac = self.sess.run(self.sy_sampled_ac, feed_dict={self.sy_ob_no: ob[None, :]}) \n",
    "        ac = ac[0]\n",
    "        acs.append(ac)\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "        # add the observation after taking a step to next_obs\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "        steps += 1\n",
    "        # If the episode ended, the corresponding terminal value is 1\n",
    "        # otherwise, it is 0\n",
    "        if done or steps > self.max_path_length:\n",
    "            terminals.append(1)\n",
    "            break\n",
    "        else:\n",
    "            terminals.append(0)\n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "            \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}\n",
    "    return path\n",
    "\n",
    "Agent.sample_trajectories = sample_trajectories\n",
    "Agent.sample_trajectory = sample_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Let's carry out the experiments.\n",
    "\n",
    "Below is the code in which we conduct experiments with the built agent. First, we initialize the environment (Set Up Env), then create an agent (Initialize Agent) and run it in the environment in a loop (Training Loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AC(\n",
    "        exp_name,\n",
    "        env_name,\n",
    "        n_iter=100, \n",
    "        gamma=1, \n",
    "        min_timesteps_per_batch=1000, \n",
    "        max_path_length=None,\n",
    "        learning_rate=5e-3,\n",
    "        num_target_updates=10,\n",
    "        num_grad_steps_per_target_update=10,\n",
    "        animate=False, \n",
    "        normalize_advantages=True,\n",
    "        seed=1,\n",
    "        n_layers=2,\n",
    "        size=64,\n",
    "        scope=\"scope\"):\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    #========================================================================================#\n",
    "    # Set Up Env\n",
    "    #========================================================================================#\n",
    "\n",
    "    # Make the gym environment\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Set random seeds\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    # Maximum length for episodes\n",
    "    max_path_length = max_path_length or env.spec.max_episode_steps\n",
    "\n",
    "    # Is this env continuous, or self.discrete?\n",
    "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "\n",
    "    # Observation and action sizes\n",
    "    ob_dim = env.observation_space.shape[0]\n",
    "    ac_dim = env.action_space.n if discrete else env.action_space.shape[0]\n",
    "\n",
    "    #========================================================================================#\n",
    "    # Initialize Agent\n",
    "    #========================================================================================#\n",
    "\n",
    "    computation_graph_args = {\n",
    "        'n_layers': n_layers,\n",
    "        'ob_dim': ob_dim,\n",
    "        'ac_dim': ac_dim,\n",
    "        'discrete': discrete,\n",
    "        'size': size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'num_target_updates': num_target_updates,\n",
    "        'num_grad_steps_per_target_update': num_grad_steps_per_target_update,\n",
    "        }\n",
    "\n",
    "    sample_trajectory_args = {\n",
    "        'animate': animate,\n",
    "        'max_path_length': max_path_length,\n",
    "        'min_timesteps_per_batch': min_timesteps_per_batch,\n",
    "        }\n",
    "\n",
    "    estimate_advantage_args = {\n",
    "        'gamma': gamma,\n",
    "        'normalize_advantages': normalize_advantages,\n",
    "        }\n",
    "    \n",
    "    agent = Agent(computation_graph_args, sample_trajectory_args, estimate_advantage_args, scope=scope) #estimate_return_args\n",
    "\n",
    "    # build computation graph\n",
    "    agent.build_computation_graph()\n",
    "\n",
    "    # tensorflow: config, session, variable initialization\n",
    "    agent.init_tf_sess()\n",
    "\n",
    "    #========================================================================================#\n",
    "    # Training Loop\n",
    "    #========================================================================================#\n",
    "\n",
    "    total_timesteps = 0\n",
    "    results = [[], []]\n",
    "    for itr in range(n_iter):\n",
    "        print(\"********** Iteration %i ************\"%itr)\n",
    "        paths, timesteps_this_batch = agent.sample_trajectories(itr, env)\n",
    "        total_timesteps += timesteps_this_batch\n",
    "\n",
    "        # Build arrays for observation, action for the policy gradient update by concatenating \n",
    "        # across paths\n",
    "        ob_no = np.concatenate([path[\"observation\"] for path in paths])\n",
    "        ac_na = np.concatenate([path[\"action\"] for path in paths])\n",
    "        re_n = np.concatenate([path[\"reward\"] for path in paths])\n",
    "        next_ob_no = np.concatenate([path[\"next_observation\"] for path in paths])\n",
    "        terminal_n = np.concatenate([path[\"terminal\"] for path in paths])\n",
    "\n",
    "        # Call tensorflow operations to:\n",
    "        # (1) update the critic, by calling agent.update_critic\n",
    "        # (2) use the updated critic to compute the advantage by, calling agent.estimate_advantage\n",
    "        # (3) use the estimated advantage values to update the actor, by calling agent.update_actor\n",
    "        agent.update_critic(ob_no, next_ob_no, re_n, terminal_n)\n",
    "        adv_n = agent.estimate_advantage(ob_no, next_ob_no, re_n, terminal_n)\n",
    "        agent.update_actor(ob_no, ac_na, adv_n)\n",
    "        \n",
    "        # Log diagnostics\n",
    "        returns = [path[\"reward\"].sum() for path in paths]\n",
    "        ep_lengths = [pathlength(path) for path in paths]\n",
    "        print(\"Time\", time.time() - start)\n",
    "        print(\"Iteration\", itr)\n",
    "        print(\"AverageReturn\", np.mean(returns))\n",
    "        print(\"StdReturn\", np.std(returns))\n",
    "        print(\"MaxReturn\", np.max(returns))\n",
    "        print(\"MinReturn\", np.min(returns))\n",
    "        print(\"EpLenMean\", np.mean(ep_lengths))\n",
    "        print(\"EpLenStd\", np.std(ep_lengths))\n",
    "        print(\"TimestepsThisBatch\", timesteps_this_batch)\n",
    "        print(\"TimestepsSoFar\", total_timesteps)\n",
    "        plot(itr, np.mean(returns))\n",
    "        results[0].append(itr)\n",
    "        results[1].append(np.mean(returns))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with several environments and different parameters: __CartPole-v0__ (num_target_updates = [1,10,100], num_grad_steps_per_target_update = [1,10,100], max_path_length = 3000).\n",
    "\n",
    "\n",
    "Let's choose the best combination for num_target_updates and num_grad_steps_per_target_update and check with these parameters __InvertedPendulum-v2__ (gamma=0.9, learning_rate=0.01, min_timesteps_per_batch=5000, size=64, max_path_length=3000), __HalfCheetah-v2__ (gamma=0.95, learning_rate=0.02, min_timesteps_per_batch=30000, size=32, max_path_length=3000). \n",
    "\n",
    "For each environment, we describe the features of the environment, construct graphs of the agent's quality of work, and comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_results = []\n",
    "\n",
    "for num_target_updates in [1, 10, 100]:\n",
    "    for num_grad_steps_per_target_update in [1, 10, 100]:\n",
    "        res = train_AC(\n",
    "                exp_name=\"CartPole-v0\",\n",
    "                env_name=\"CartPole-v0\",\n",
    "                num_target_updates=num_target_updates,\n",
    "                num_grad_steps_per_target_update=num_grad_steps_per_target_update,\n",
    "                scope='scope' + 'CartPole_v0' + str(num_target_updates) + str(num_grad_steps_per_target_update)\n",
    "        )\n",
    "        cartpole_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17, 20))\n",
    "\n",
    "for i_col in range(3):\n",
    "    for j_col in range(3):\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.subplot(3, 3, 3*i_col+j_col+1)\n",
    "        x_col = cartpole_results[i_col+j_col][0]\n",
    "        y_col = cartpole_results[i_col+j_col][1]\n",
    "        plt.plot(x_col, y_col)\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel('reward')\n",
    "        plt.title('num_target_updates: {}\\nnum_grad_steps_per_target_update: {}'.format(10**i_col, 10**j_col))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs show that the maximum reward that we can potentially get in this environment is 200 cu Let us analyze some of them individually:\n",
    "\n",
    "**1.1.** Really not quite good, even in 100 eras the result is unsatisfactory.\n",
    "\n",
    "**1.10.** Ð¸ **10.1** Here, the results are already much better â€“ at nearly 25 iterations we are approaching the desired reward, but the graph is fluctuating\n",
    "\n",
    "**10.10.** **1.100.** **100.1.** In this case, the reward begins to approach a reward of 200 a little later (at about 40 iterations), while the graph is almost steady.\n",
    "\n",
    "**100.10.** **10.100.** Everything is fine, but at 80 iterations, reward goes down sharply and it is not clear whether it will come back or not.\n",
    "\n",
    "**100.100.** Everything is great here: at about 15 iterations, the reward reaches the desired value and the graph generally does not flactuate - the best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InvertedPendulum-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see the best values of parameters num_target_updates and num_grad_steps_per_target_update are 100 and 100. \n",
    "# But it's too long and 10 and 10 give us almost the same reward so that\n",
    "num_target_updates = 10\n",
    "num_grad_steps_per_target_update = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = train_AC(\n",
    "                exp_name=\"InvertedPendulum-v2\",\n",
    "                env_name=\"InvertedPendulum-v2\",\n",
    "                n_iter=100,\n",
    "                gamma=0.9,\n",
    "                min_timesteps_per_batch=5000,\n",
    "                max_path_length=3000,\n",
    "                learning_rate=0.01,\n",
    "                num_target_updates=num_target_updates,\n",
    "                num_grad_steps_per_target_update=num_grad_steps_per_target_update,\n",
    "                animate=False,\n",
    "                normalize_advantages=False,\n",
    "                seed=1,\n",
    "                n_layers=2,\n",
    "                size=64,\n",
    "                scope='sc_____' + 'InvertedPendulum-v2'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very unstable chart, the values of the rewards are very different at different iterations. It can be concluded that models in this environment need to be trained for a very long time before convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HalfCheetah-v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3 = train_AC(\n",
    "                exp_name=\"HalfCheetah-v2\",\n",
    "                env_name=\"HalfCheetah-v2\",\n",
    "                n_iter=100,\n",
    "                gamma=0.95,\n",
    "                min_timesteps_per_batch=5000,\n",
    "                max_path_length=3000,\n",
    "                learning_rate=0.02,\n",
    "                num_target_updates=num_target_updates,\n",
    "                num_grad_steps_per_target_update=num_grad_steps_per_target_update,\n",
    "                animate=False,\n",
    "                normalize_advantages=False,\n",
    "                seed=1,\n",
    "                n_layers=2,\n",
    "                size=32,\n",
    "                scope='sc______' + 'InvertedPendulum-v2'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, everything works quite well: the reward grows very quickly, the values of the reward do not fluctuate from iteration to iteration, quick convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
